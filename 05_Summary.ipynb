{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709c2459-a019-41df-b92f-1fea725ffea2",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c575e-e74c-466b-a6f0-023a78f3a050",
   "metadata": {},
   "source": [
    "# Project 3.05 - Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd069b-f5d8-4431-8ad5-3c37ba8bda3d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "\n",
    "- [Background](#Background)\n",
    "\n",
    "- [Datasets and Background](#Datasets-and-Background)\n",
    "\n",
    "- [Our Model Choice and Final Conclusions](#Our-Model-Choice-and-Final-Conclusions) \n",
    "\n",
    "- [Recommendations](#Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1192ac-356c-4676-9507-a3550c697eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bcb66-73b3-4f8d-8ff5-5e085fd08899",
   "metadata": {},
   "source": [
    "You are a analyst of a market research team at LSB Industries that has been tasked to introduce the team to machine learning programming, and as an teaching exercise, you arre looking to predict which subreddit a post belongs too. The chosen subreddits for this exercise are R/Finance and R/Economics. The data approach would be reproduceable for further projects utilizing python and reddit and is a beginning stage of your company's move to social media analytics.\n",
    "\n",
    "The project is expected to show a classical modeling approach, and would assist in solving the problem case of your company being unable to predict social media data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa178460-d1f0-4de4-9c58-c8ff1f871742",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765944a-2130-495d-ba15-6347a98eaef6",
   "metadata": {},
   "source": [
    "Reddit is a social media tool that draws in over over [430](https://en.wikipedia.org/wiki/Reddit) million users around the world on a monthly basis, who meet in forums called \"subreddits\", which are tailored to whatever interest brought them there. For example, subreddits like R/Dating Advice, or R/Personal Finance help users congregate, ask questions, and discuss new information. Some subreddits are more video based in nature, catering to those who just want to watch videos or post memes.\n",
    "\n",
    "Reddit's works like a bulletin board system, where users \"upvote\" or comment on certain topics, contributing to the discussion and pushing the post up higher on the bulletin board. Posts with the most upvotes are pushed to the front page of the site, before being recycled to the background after 24 hours, leading to a continous inflow of new content.\n",
    "\n",
    "Reddit's reach and content serves as an opportunity for companies to draw in significant amounts of qualitative text-based data, and siloed extremely well, allowing for content to be very specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adafa6c-64dd-4c86-bd9f-016234e0309c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Datasets and Background\n",
    "\n",
    "\n",
    "* [`reddit_posts.csv`](./Data/reddit_posts.csv): Cleaned reddit posts from Reddit Finance and Reddit Economics subreddits;([Finance source](https://www.reddit.com/r/finance/)  | [Economics source](https://www.reddit.com/r/economics/)))\n",
    "\n",
    "The data set contains 20,000 posts from the Reddit Finance and Reddit Economics subreddits. These posts are text_only, and largely cover news, events, and discussion regarding economic and financial news around the world. A detailed look at the data is below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f7904-2a18-47dc-8127-8a434c038f21",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95310a-5c2f-43cd-a0f8-557572e43ca4",
   "metadata": {},
   "source": [
    "\n",
    "![](./Images/data_Dict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a84e52-4ea8-407e-a7b7-c3ba9d5011d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805b5c4c-892d-434b-aa0b-50d0b78863d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42696cb-4944-42d8-9fbe-3a5d58771e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./Data/Reddit_Posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4408f5d-1790-41f6-b0f2-0a871c12562b",
   "metadata": {},
   "source": [
    "## Model Setup and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388f02e-233f-49bf-b149-c7badf877767",
   "metadata": {},
   "source": [
    "Over the course of this project we ran several different models to ascertain if any of them were a stronger predicter of our target variable.\n",
    "    \n",
    "**Some main assumptions prior to modeling:**\n",
    "    \n",
    "- Null Hypothesis is .5 or 50% accuracy, as we are trying to predict between to subreddit data samples that are identical in size.\n",
    "\n",
    "- Our dependent variable is whether or not a post belongs to the Finance subreddit.\n",
    "\n",
    "- Our independent variable is the text body from the title of the post, vectorized and cleaned using a Vectorizer\n",
    "\n",
    "\n",
    "**Tested Models:**\n",
    "\n",
    "- Logistic Regression \n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- KNN- Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65b80e-6695-4b18-991c-65ef57e39413",
   "metadata": {},
   "source": [
    "The overall scoring of these models is listed below. Please note the following acroynms:\n",
    "\n",
    "- GRD = Grid-searched\n",
    "- Vect= Count Vectorized\n",
    "- Tfid= Tfidf Vectorized\n",
    "- LogR = Logistic Regression\n",
    "- MNB = Multinomial Naive Bayes\n",
    "- RF - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632dd11-296b-47e5-927e-b48a0bd93f02",
   "metadata": {},
   "source": [
    "\n",
    "![](./Images/Model_Results.png)\n",
    "\n",
    "![](./Images/Metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4c72d-8f0b-488d-9349-2158eba45a5b",
   "metadata": {},
   "source": [
    "- Across all models, we made accurate predictions that were above our Null Hypothesis. Therefore, we will accept the model we choose. \n",
    "\n",
    "- All models were vectorized as a base assumption, while we did 3 gridsearches to determine the best parameters for our Multinomial, Random Forest, and K Neighbors models.\n",
    "\n",
    "\n",
    "- There is overfitting that is consistent across most models, which lends some indication that we can further improve our modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36feb3-cc0c-4869-9e45-22f90de8474d",
   "metadata": {},
   "source": [
    "## Our Model Choice and Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88145c-6147-47ec-a831-7ad73af9324f",
   "metadata": {},
   "source": [
    "As we were in the process, our choice came down to two models. Our Tfidf vectorized K-neighbors (with grid search), or Tfidf vectorized Random Forest (with grid search).\n",
    "\n",
    "**Ultimately, we chose our K-neighbors model, for one primary reason:**\n",
    "\n",
    "- Our random forest is severely overfit (98% train score vs. 75% test score). Conservatively, the K-Neighbors is more appealing as both score 74%, meaning we have a greater degree of fit on unseen data.\n",
    "\n",
    "\n",
    "However, we recognize that K-neighbors, while a good model, has limitations that may necessitate further exploration into the other models. \n",
    "\n",
    "For one, there is no ability to pull feature coefficients in K-Neighbors. This may limit some analysis, given we can't see how individual words are influencing the model. As well, The recall scoring -- Our ability to predict true positives, is significantly lower than the other models. This would mean that our model doesn't do as good of a job predicting if a post belongs to Reddit Finance. These factors alone give us some degree of pause in being fully reliant on the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013baeb-c877-4604-9c66-1dae1a7ab039",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa84f4b-ea94-4116-aef8-d3b5e192f279",
   "metadata": {},
   "source": [
    "Recommendations wise, we suggest the following improvements be made to the model, as they wil improve our analytics potential:\n",
    "\n",
    "**Add reddit comments from the respective subreddits to the model flow, so that we can evaluate both at once:**\n",
    "\n",
    "- We were not able to draw in comment data given time constraints in this project, but that doesn't dimish the analytics potential that exists there. In fact, it can be argued that even more information can be drawn them. The model if fairly reproducible in it's current state, so modifications using existing code can be completed relatively quickly once the API pull is created.\n",
    "\n",
    "\n",
    "**Code that allows one to search for all mentions of a specified word and pull out the most upvoted posts and comments:**\n",
    "\n",
    "- Reddit it built on an upvote system, and many information based subreddits will draw the most insightful information and data up to the top. Given the nature of our market research department, implementing data extraction processes that allow for us to filter information by specific words or phrases can give us a better ability to catelog immediate information that may be business-critical.\n",
    "\n",
    "**Reproducible sentiment analysis given a beginning date. (Capturing and evaluating most recent posts)**\n",
    "\n",
    "- Sentiment analysis is a potential boon for our company, and can help us see how customers are responding to advertising campaigns, competitor practices, and cultural and environmental shifts. The development of a sentiment analysis that allows us to acquire recent information can possibily give us an operational edge over the rest of the market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
